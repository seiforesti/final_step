# # train_hybrid_model.py
# import json
# import os
# import pandas as pd
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.pipeline import Pipeline
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, precision_recall_fscore_support
# import joblib

# # üìÇ Chemin de sauvegarde du mod√®le
# MODEL_PATH = "classifiers/ml_models/hybrid_model.pkl"

# # üî¢ Chargement des donn√©es
# # Exemple CSV attendu: column_name,category
# DATA_FILE = "app/api/classifiers/ml_models/data/training_data.csv"
# if not os.path.exists(DATA_FILE):
#     raise FileNotFoundError(f"Fichier d'entra√Æment manquant: {DATA_FILE}")

# print("‚ú® Chargement des donn√©es...")
# df = pd.read_csv(DATA_FILE)

# # Nettoyage
# if "column_name" not in df.columns or "category" not in df.columns:
#     raise ValueError("Le fichier doit contenir les colonnes 'column_name' et 'category'")

# X = df["column_name"].astype(str)
# y = df["category"].astype(str)

# # üìà Split train/test
# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# # üîß Pipeline avec vectorisation
# model = Pipeline([
#     ("vectorizer", CountVectorizer()),
#     ("classifier", RandomForestClassifier(n_estimators=100, random_state=42))
# ])

# print("‚öñÔ∏è Entra√Ænement du mod√®le...")
# model.fit(X_train, y_train)

# # ‚úÖ √âvaluation
# print("\n‚úçÔ∏è Rapport de performance:")
# y_pred = model.predict(X_test)
# print(classification_report(y_test, y_pred))

# # üíæ Sauvegarde
# os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
# joblib.dump(model, MODEL_PATH)
# print(f"‚úîÔ∏è Mod√®le sauvegard√© dans: {MODEL_PATH}")





# precision, recall, f1, support = precision_recall_fscore_support(
# y_test, y_pred, average="weighted", zero_division=0
# )

# metrics = {
# "precision": round(precision, 4),
# "recall": round(recall, 4),
# "f1_score": round(f1, 4),
# "support": int(support)
# }

# metrics_path = os.path.join("app", "api", "classifiers", "ml_models", "metrics.json")
# os.makedirs(os.path.dirname(metrics_path), exist_ok=True)

# with open(metrics_path, "w") as f:
#     json.dump(metrics, f, indent=2)

# print(f"‚úÖ M√©triques sauvegard√©es dans {metrics_path}")







# train_hybrid_model.py

# import json
# import os
# import pandas as pd
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.pipeline import Pipeline
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, precision_recall_fscore_support
# import joblib

# # üìÇ Chemins
# DATA_FILE = "app/api/classifiers/ml_models/data/training_data.csv"
# MODEL_PATH = "app/api/classifiers/ml_models/hybrid_model.pkl"
# METRICS_PATH = "app/api/classifiers/ml_models/metrics.json"

# # üß† Charger les donn√©es
# if not os.path.exists(DATA_FILE):
#     raise FileNotFoundError(f"Fichier d'entra√Ænement manquant: {DATA_FILE}")

# print("‚ú® Chargement des donn√©es...")
# df = pd.read_csv(DATA_FILE)

# if "column_name" not in df.columns or "category" not in df.columns:
#     raise ValueError("Le fichier doit contenir les colonnes 'column_name' et 'category'")

# X = df["column_name"].astype(str)
# y = df["category"].astype(str)

# # üîÄ Split train/test
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, stratify=y, test_size=0.2, random_state=42
# )

# # ‚öôÔ∏è Pipeline avec CountVectorizer + RandomForest
# model = Pipeline([
#     ("vectorizer", CountVectorizer(lowercase=True, ngram_range=(1, 2))),
#     ("classifier", RandomForestClassifier(n_estimators=100, random_state=42))
# ])

# print("‚öñÔ∏è Entra√Ænement du mod√®le...")
# model.fit(X_train, y_train)

# # ‚úÖ √âvaluation
# print("\n‚úçÔ∏è Rapport de performance:")
# y_pred = model.predict(X_test)
# print(classification_report(y_test, y_pred))

# # üìä M√©triques
# precision, recall, f1, support = precision_recall_fscore_support(
#     y_test, y_pred, average="weighted", zero_division=0
# )

# metrics = {
#     "precision": round(precision, 4),
#     "recall": round(recall, 4),
#     "f1_score": round(f1, 4),
#     "support": int(support) if not isinstance(support, (list, tuple)) else int(sum(support))
# }

# # üíæ Sauvegardes
# os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
# joblib.dump(model, MODEL_PATH)
# print(f"‚úîÔ∏è Mod√®le sauvegard√© dans: {MODEL_PATH}")

# os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)
# with open(METRICS_PATH, "w") as f:
#     json.dump(metrics, f, indent=2)
# print(f"üìä M√©triques sauvegard√©es dans: {METRICS_PATH}")

#################################################################################################################################""""""

# ‚úÖ train_hybrid_model.py (mise √† jour automatique + export m√©triques)
# import json
# import os
# import pandas as pd
# import joblib
# from sklearn.pipeline import Pipeline
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, precision_recall_fscore_support
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.ensemble import RandomForestClassifier

# # üìÇ Chemins
# DATA_FILES = [
#     "app/api/classifiers/ml_models/data/training_data.csv",
#     "app/api/classifiers/ml_models/data/augmented_training_data.csv"
# ]
# MODEL_PATH = "app/api/classifiers/ml_models/hybrid_model.pkl"
# METRICS_PATH = "app/api/classifiers/ml_models/metrics.json"

# # ‚úÖ Chargement des donn√©es
# dfs = []
# for data_file in DATA_FILES:
#     if not os.path.exists(data_file):
#         raise FileNotFoundError(f"Fichier d'entra√Ænement manquant: {data_file}")
#     print(f"‚ú® Chargement des donn√©es depuis {data_file}...")
#     dfs.append(pd.read_csv(data_file))

# df = pd.concat(dfs, ignore_index=True)

# if "column_name" not in df.columns or "category" not in df.columns:
#     raise ValueError("Les fichiers CSV doivent contenir les colonnes 'column_name' et 'category'")

# X = df["column_name"].astype(str)
# y = df["category"].astype(str)

# # ‚úÖ S√©paration train/test
# X_train, X_test, y_train, y_test = train_test_split(
#     X, y, stratify=y, test_size=0.2, random_state=42
# )

# # ‚úÖ Pipeline Scikit-learn
# model = Pipeline([
#     ("vectorizer", CountVectorizer(lowercase=True, ngram_range=(1, 2))),
#     ("classifier", RandomForestClassifier(n_estimators=100, random_state=42))
# ])

# print("‚öôÔ∏è Entra√Ænement du mod√®le...")
# model.fit(X_train, y_train)

# # ‚úÖ √âvaluation
# y_pred = model.predict(X_test)
# print("\n‚úçÔ∏è Rapport:")
# print(classification_report(y_test, y_pred))

# # ‚úÖ M√©triques
# precision, recall, f1, support = precision_recall_fscore_support(
#     y_test, y_pred, average="weighted", zero_division=0
# )

# metrics = {
#     "precision": round(precision, 4),
#     "recall": round(recall, 4),
#     "f1_score": round(f1, 4),
#     "support": int(support) if support is not None else 0
# }

# # üíæ Sauvegarde du mod√®le
# os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
# joblib.dump(model, MODEL_PATH)
# print(f"‚úÖ Mod√®le sauvegard√© √† {MODEL_PATH}")

# # üíæ Sauvegarde des m√©triques
# os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)
# with open(METRICS_PATH, "w") as f:
#     json.dump(metrics, f, indent=2)
# print(f"üìä M√©triques sauvegard√©es √† {METRICS_PATH}")

# # ‚úÖ Fonction pour entra√Æner et sauvegarder
# def train_and_save():
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     precision, recall, f1, support = precision_recall_fscore_support(
#         y_test, y_pred, average="weighted", zero_division=0
#     )
#     metrics = {
#         "precision": round(precision, 4),
#         "recall": round(recall, 4),
#         "f1_score": round(f1, 4),
#         "support": int(support) if support is not None else 0
#     }
#     joblib.dump(model, MODEL_PATH)
#     with open(METRICS_PATH, "w") as f:
#         json.dump(metrics, f, indent=2)
#     return {"model_path": MODEL_PATH, "metrics_path": METRICS_PATH}
####################################################################################################################################
# ‚úÖ train_hybrid_model.py (version embeddings FastText + RandomForest)
import os
import json
import pandas as pd
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, precision_recall_fscore_support

# üìÇ Chemins
DATA_FILES = [
    "app/api/classifiers/ml_models/data/training_data.csv",
    "app/api/classifiers/ml_models/data/augmented_training_data.csv"
]
MODEL_PATH = "app/api/classifiers/ml_models/hybrid_model.pkl"
METRICS_PATH = "app/api/classifiers/ml_models/metrics.json"

# ‚úÖ Chargement des donn√©es
dfs = []
for data_file in DATA_FILES:
    if os.path.exists(data_file):
        print(f"‚ú® Chargement des donn√©es depuis {data_file}...")
        dfs.append(pd.read_csv(data_file))

df = pd.concat(dfs, ignore_index=True)

if "column_name" not in df.columns or "category" not in df.columns:
    raise ValueError("Les fichiers CSV doivent contenir les colonnes 'column_name' et 'category'")

X_raw = df["column_name"].astype(str)
y = df["category"].astype(str)

# ‚úÖ Encodage des noms de colonnes en vecteurs avec SentenceTransformer
print("üì• Chargement du mod√®le SentenceTransformer...")
st_model = SentenceTransformer('all-MiniLM-L6-v2')

print("üì• Encodage des colonnes en embeddings...")
X_vec = st_model.encode(X_raw.tolist(), show_progress_bar=True)

# ‚úÖ Split train/test
X_train, X_test, y_train, y_test = train_test_split(X_vec, y, stratify=y, test_size=0.2, random_state=42)

# ‚úÖ Mod√®le RandomForest
model = RandomForestClassifier(n_estimators=100, random_state=42)
print("‚öôÔ∏è Entra√Ænement du mod√®le...")
model.fit(X_train, y_train)

# ‚úÖ √âvaluation
y_pred = model.predict(X_test)
print("\n‚úçÔ∏è Rapport:")
print(classification_report(y_test, y_pred))

# ‚úÖ M√©triques
precision, recall, f1, support = precision_recall_fscore_support(
    y_test, y_pred, average="weighted", zero_division=0
)

metrics = {
    "precision": round(precision, 4),
    "recall": round(recall, 4),
    "f1_score": round(f1, 4),
    "support": int(support) if support is not None else 0
}

# üíæ Sauvegarde du mod√®le
os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
joblib.dump(model, MODEL_PATH)
print(f"‚úÖ Mod√®le sauvegard√© √† {MODEL_PATH}")

# üíæ Sauvegarde des m√©triques
os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)
with open(METRICS_PATH, "w") as f:
    json.dump(metrics, f, indent=2)
print(f"üìä M√©triques sauvegard√©es √† {METRICS_PATH}")

# ‚úÖ Fonction pour r√©utiliser en watcher ou API
def train_and_save():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    precision, recall, f1, support = precision_recall_fscore_support(
        y_test, y_pred, average="weighted", zero_division=0
    )
    metrics = {
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1_score": round(f1, 4),
        "support": int(support) if support is not None else 0
    }
    joblib.dump(model, MODEL_PATH)
    with open(METRICS_PATH, "w") as f:
        json.dump(metrics, f, indent=2)
    return {"model_path": MODEL_PATH, "metrics_path": METRICS_PATH}
